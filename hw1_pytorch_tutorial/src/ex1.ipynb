{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f8a943",
   "metadata": {},
   "source": [
    "# Exercise 1: Tensor basics \n",
    "In this exercise you will learn the basics of tensor creation, manipulation, indexing, broadcasting, vectorization, einsum, and attention masking fundamentals. These basics are important for understanding any complex implementation later on so make sure you understand them well.\n",
    "\n",
    "**To complete this exercise fill in all TODOs in the functions below.** \n",
    "\n",
    "Make sure to check the output of your function and whether or not it fulfills the requirements outlined in the function definition. Do NOT change the function signature or name since we will be running checks on your functions during grading.\n",
    "\n",
    "### Shape legend used in this notebook\n",
    "- `B`: batch size\n",
    "- `T`: sequence length / time\n",
    "- `D`: feature dimension\n",
    "- `H`: number of attention heads\n",
    "- `Dh`: per-head feature dimension\n",
    "\n",
    "### Debugging tip: what to print\n",
    "When you get a shape error, print:\n",
    "- `x.shape`, `x.dtype`, `x.device`\n",
    "- `x.is_contiguous()` (important for `view`)\n",
    "For masks also print:\n",
    "- `mask.shape`, `mask.dtype`, `mask.sum()` and a small slice like `mask[0, :10]`\n",
    "\n",
    "### Reproducibility tip: seeding in PyTorch\n",
    "Many operations in deep learning involve randomness (e.g., initializing model weights, shuffling data, dropout, random augmentations).\n",
    "**Seeding** sets the starting state of PyTorch’s random number generator so that these random choices become **repeatable**.\n",
    "\n",
    "- If you set the same seed and run the same code again, you should get the same *random* tensors / initial weights.\n",
    "- If you don’t set a seed, results can vary between runs.\n",
    "\n",
    "Common usage: `torch.manual_seed(seed)`\n",
    "\n",
    "Note: even with fixed seeds, some GPU operations can still be non-deterministic due to performance optimizations. For this assignment, seeding is mainly to make debugging easier and to ensure everyone can reproduce the same intermediate results. If you are given a seed, make sure to use it when creating tensors or performing other operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d374d",
   "metadata": {},
   "source": [
    "## Tensor creation\n",
    "This warmup exercise teaches you how to create tensors with different shapes and values. A few details about tensor creation that are good to know:\n",
    "- `torch.tensor([...])` infers dtype from Python values (ints → integer tensor, floats → float tensor).\n",
    "- `torch.arange(start, end)` is **end-exclusive**.\n",
    "- `torch.linspace(start, end, steps)` is **end-inclusive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ae4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b781b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(data, dtype: torch.dtype | None = None, device: torch.device | str | None = None) -> torch.Tensor:\n",
    "    \"\"\" Create a tensor from Python data (list/tuple/nested lists). \"\"\"\n",
    "    return torch.tensor(data,dtype=dtype,device=device)\n",
    "\n",
    "x = make_tensor([[1, 2], [3, 4]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b76bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zeros(shape: Sequence[int], dtype: torch.dtype | None = None, device: torch.device | str | None = None) -> torch.Tensor:\n",
    "    \"\"\"Create a tensor filled with zeros.\"\"\"\n",
    "    return torch.zeros(shape, dtype=dtype, device=device)\n",
    "\n",
    "z = make_zeros((2, 3), dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758dfb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ones_like(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Create a tensor of ones with the same shape, dtype, and device as x. \"\"\"\n",
    "    return torch.ones_like(x, dtype=x.dtype, device=x.device)\n",
    "\n",
    "base = torch.randn(2, 3, dtype=torch.float32)\n",
    "ones = make_ones_like(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b764cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arange(start: int, end: int, step: int = 1, dtype: torch.dtype | None = None, device: torch.device | str | None = None) -> torch.Tensor:\n",
    "    \"\"\"Create a 1D tensor containing values [start, start+step, ..., < end].\"\"\"\n",
    "    return torch.arange(start, end, step=step, dtype=dtype, device=device)\n",
    "\n",
    "ar = make_arange(0, 5, 2, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd936a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linspace(start: float, end: float, steps: int, dtype: torch.dtype | None = None, device: torch.device | str | None = None) -> torch.Tensor:\n",
    "    \"\"\"Create a 1D tensor with evenly spaced values from start to end (inclusive).\"\"\"\n",
    "    return torch.linspace(start, end, steps, dtype=dtype,device=device)\n",
    "\n",
    "ls = make_linspace(0.0, 1.0, steps=5, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60f092d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_randn(shape: Sequence[int], seed: int | None = None, dtype: torch.dtype | None = None, device: torch.device | str | None = None) -> torch.Tensor:\n",
    "    \"\"\"Create a tensor filled with values from a standard normal distribution.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    return torch.randn(shape, dtype=dtype, device=device)\n",
    "\n",
    "a = make_randn((2, 3), seed=123, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c6eb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_dtype_and_move(x: torch.Tensor, device: torch.device, dtype: torch.dtype) -> torch.Tensor:\n",
    "    \"\"\"Convert tensor dtype and move to device.\"\"\"\n",
    "    x=x.type(dtype)\n",
    "    x=x.to(device)\n",
    "    return x\n",
    "\n",
    "casted = cast_dtype_and_move(torch.tensor([1, 2, 3]), torch.device(\"cpu\"), torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98566de",
   "metadata": {},
   "source": [
    "## Shape manipulation\n",
    "Now that we covered the basic tensor creation schemes, we want to focus on shape manipulation. Understanding the difference between these mechanisms is key for building larger systems and many people still get it wrong. \n",
    "The core ideas to understand are:\n",
    "- **Contiguous tensors** store data in a single, row-major memory layout.\n",
    "- Many ops (especially slicing like `x[:, ::2]`, `transpose`, `permute`) often create **non-contiguous** tensors (no copy but different strides).\n",
    "- `view(...)` is **zero-copy** but typically requires **contiguous** memory → may throw an error.\n",
    "- `reshape(...)` tries to return a view, but if the tensor is non-contiguous it will **allocate/copy**.\n",
    "- `contiguous()` forces a contiguous copy when the tensor isn’t contiguous.\n",
    "\n",
    "If you *need* a view after reordering dims: call `x = x.contiguous()` first (this makes a contiguous copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6def6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_tensor(x: torch.Tensor, new_shape: Sequence[int]) -> torch.Tensor:\n",
    "    \"\"\"Reshape tensor to new_shape (may return a view or a copy).\"\"\"\n",
    "    return x.reshape(new_shape)\n",
    "\n",
    "x = torch.arange(6)\n",
    "y = reshape_tensor(x, (2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b508ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "def view_tensor(x: torch.Tensor, new_shape: Sequence[int]) -> torch.Tensor:\n",
    "    \"\"\"View tensor as new_shape (requires contiguous memory and doesn't allocate new memory for the tensor data).\"\"\"\n",
    "    y = reshape_tensor(x, new_shape)\n",
    "    y=y.contiguous()\n",
    "    return y\n",
    "\n",
    "y_view = view_tensor(x, (2, 3))\n",
    "print(y_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59fe1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_from_dim(x: torch.Tensor, start_dim: int = 0) -> torch.Tensor:\n",
    "    \"\"\"Flatten a tensor starting from start_dim into a single dimension.\"\"\"\n",
    "    return x.flatten()\n",
    "\n",
    "x2 = torch.randn(2, 3, 4)\n",
    "flat = flatten_from_dim(x2, start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c733b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_singleton_dim(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    \"\"\"Insert a size-1 dimension at position dim.\"\"\"\n",
    "    return torch.unsqueeze(x, dim)\n",
    "\n",
    "x3 = torch.randn(5, 7)\n",
    "x3s = add_singleton_dim(x3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9552b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_singleton_dims(x: torch.Tensor, dim: int | None = None) -> torch.Tensor:\n",
    "    \"\"\"Remove size-1 dimensions.\"\"\"\n",
    "    return torch.squeeze(x, 0 if dim==None else dim)\n",
    "\n",
    "x4 = torch.randn(2, 1, 3)\n",
    "x4s = remove_singleton_dims(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ad8f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_last_two(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Swap the last two dimensions of x.\"\"\"\n",
    "    return x6.transpose(-1,-2)\n",
    "\n",
    "x6 = torch.randn(2, 3, 4)\n",
    "x6t = transpose_last_two(x6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a26228c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_bhwc_to_bchw(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Convert (B, H, W, C) tensor into (B, C, H, W).\"\"\"\n",
    "    return torch.permute(x, (0, 3, 1, 2))\n",
    "\n",
    "x7 = torch.randn(8, 32, 32, 3)\n",
    "x7p = permute_bhwc_to_bchw(x7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfd919ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_contiguous(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Check if tensor is contiguous and if not make contiguous.\"\"\"\n",
    "    if not x.is_contiguous():\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "x8 = torch.randn(4, 6)[:, ::2]\n",
    "x8c = make_contiguous(x8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76616564",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "Now that we know how to create tensors and manipulate them we need to understand how we can extract certain components from them using indexing. \n",
    "- Basic slicing (`x[a:b]`) returns a view when possible.\n",
    "- “Fancy” indexing (lists/tensors of indices) usually allocates a new tensor.\n",
    "- In-place vs out-of-place matters: if a function says “return a copy, leave the input unchanged”, you need `clone()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00724376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_rows(x: torch.Tensor, start: int, end: int) -> torch.Tensor:\n",
    "    \"\"\"Slice rows in a 2D tensor: x[start:end, :].\"\"\"\n",
    "    return x[start:end, :]\n",
    "\n",
    "x = torch.arange(12).reshape(4, 3)\n",
    "rows = slice_rows(x, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "703bcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(x: torch.Tensor, cols: Sequence[int]) -> torch.Tensor:\n",
    "    \"\"\"Select specific columns from a 2D tensor.\"\"\"\n",
    "    return x[:,cols]\n",
    "\n",
    "cols = select_columns(x, [0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c08ff8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diagonal(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Get the diagonal of a 2D tensor.\"\"\"\n",
    "    return torch.diag(x)\n",
    "\n",
    "d = get_diagonal(torch.tensor([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97eff27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_subtensor(x: torch.Tensor, row_idx: int, col_idx: int, value: float) -> torch.Tensor:\n",
    "    \"\"\"Return a copy of x where x[row_idx, col_idx] is set to value.\"\"\"\n",
    "    # TODO: implement\n",
    "    x2 = torch.clone(x)\n",
    "    x2[row_idx, col_idx] = value\n",
    "    return x2\n",
    "\n",
    "base = torch.zeros(2, 2)\n",
    "out = set_subtensor(base, 0, 1, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c92ac1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_rows(x: torch.Tensor, row_indices: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Gather (concat) rows from x using row_indices.\"\"\"\n",
    "    return x2[idx,:]\n",
    "\n",
    "x2 = torch.tensor([[10, 11], [20, 21], [30, 31]])\n",
    "idx = torch.tensor([2, 0])\n",
    "gathered = gather_rows(x2, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0cd0b7",
   "metadata": {},
   "source": [
    "## Broadcasting and reducing\n",
    "Now we're covering a pytorch mechanism that lets you apply elementwise ops without using python loops. It's important to understand how it works to trace your shapes in complicated systems. The broadcasting rules to know are:\n",
    "- Dimensions align from the **right**.\n",
    "- A dimension can broadcast if it’s equal or one of them is **1**.\n",
    "\n",
    "### Reduction ops and `keepdim`\n",
    "\n",
    "When you reduce over a dimension (e.g. `sum`, `mean`, `max`), PyTorch can either:\n",
    "\n",
    "- **remove** the reduced dimension (`keepdim=False`, default), or\n",
    "- **keep** it as size 1 (`keepdim=True`)\n",
    "\n",
    "Keeping the dimension is often helpful because it makes broadcasting back “just work”.\n",
    "\n",
    "#### Shape diagram examples\n",
    "\n",
    "Assume `x` has shape `(B, T, D)`:\n",
    "\n",
    "**Sum over time**\n",
    "- `x.sum(dim=1)` → shape `(B, D)`\n",
    "- `x.sum(dim=1, keepdim=True)` → shape `(B, 1, D)`\n",
    "\n",
    "**Mean over features**\n",
    "- `x.mean(dim=2)` → shape `(B, T)`\n",
    "- `x.mean(dim=2, keepdim=True)` → shape `(B, T, 1)`\n",
    "\n",
    "#### Why `keepdim=True` helps with broadcasting\n",
    "\n",
    "Example: center `x` by subtracting the mean over `T`\n",
    "\n",
    "- If `m = x.mean(dim=1)` has shape `(B, D)`, then `x - m` **fails** (shapes `(B,T,D)` and `(B,D)` don't align).\n",
    "- If `m = x.mean(dim=1, keepdim=True)` has shape `(B,1,D)`, then `x - m` **works** via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "541c7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_over_dim(x: torch.Tensor, dim: int, keepdim: bool = False) -> torch.Tensor:\n",
    "    \"\"\"Sum tensor values along dimension dim.\"\"\"\n",
    "    return x.sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "x = torch.ones(2, 3)\n",
    "y = sum_over_dim(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd78da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_over_dim(x: torch.Tensor, dim: int, keepdim: bool = False) -> torch.Tensor:\n",
    "    \"\"\"Mean along dimension dim.\"\"\"\n",
    "    return x.mean(dim=dim, keepdim=keepdim)\n",
    "\n",
    "x2 = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "y2 = mean_over_dim(x2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eec32da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_over_dim(x: torch.Tensor, dim: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Max values and argmax indices along dimension dim.\"\"\"\n",
    "    return x.max(dim=dim), torch.argmax(x, dim=dim)\n",
    "\n",
    "x3 = torch.tensor([[1.0, 5.0], [3.0, 2.0]])\n",
    "values, idx = max_over_dim(x3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34c7d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_over_dim(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    \"\"\"Argmax indices along dimension dim.\"\"\"\n",
    "    torch.argmax(x, dim=dim)\n",
    "\n",
    "idx2 = argmax_over_dim(x3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5acbc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_add_vector(x: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Add a vector v to each row of a 2D tensor x using broadcasting.\"\"\"\n",
    "    return x4 + v\n",
    "\n",
    "x4 = torch.zeros(3, 2)\n",
    "v = torch.tensor([10.0, 20.0])\n",
    "y4 = broadcast_add_vector(x4, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2629b",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "We want to avoid slow (due to per-iteration overhead) python loops as much as possible and pytorch gives us many tools to avoid it. We cover these basics:\n",
    "- `cat` vs `stack` (concatenate existing dims vs create a new dim)\n",
    "- `repeat` vs `expand`\n",
    "- `scatter_add` / `index_add` for accumulation\n",
    "- `where` for conditional selection\n",
    "\n",
    "### `expand` vs `repeat`\n",
    "\n",
    "- `repeat(...)` **copies** data → larger tensor with independent storage.\n",
    "- `expand(...)` **does not copy** data → it creates a *view* with clever strides.\n",
    "\n",
    "This has two important implications:\n",
    "\n",
    "1) `expand` only works when expanding a **size-1 dimension** (broadcasting a singleton).\n",
    "2) The expanded tensor may have **many positions pointing to the same memory**.  \n",
    "   Modifying the expanded tensor can therefore produce surprising results (multiple rows change).\n",
    "\n",
    "Rule of thumb:\n",
    "- Use `expand` for read-only broadcasting.\n",
    "- Use `repeat` if you truly need independent copies.\n",
    "\n",
    "\n",
    "NOTE: We implore you to write your own quick checks from now on for calling the functions and checking their output. As before you are still required to fill in the TODOs in each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d9f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tensors(tensors: Sequence[torch.Tensor], dim: int = 0) -> torch.Tensor:\n",
    "    \"\"\"Concatenate tensors along dim. NOTE: This will always allocate new memory\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e197f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_tensors(tensors: Sequence[torch.Tensor], dim: int = 0) -> torch.Tensor:\n",
    "    \"\"\"Stack tensors along a new dimension dim.\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b20f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_tensor(x: torch.Tensor, repeats: Sequence[int]) -> torch.Tensor:\n",
    "    \"\"\"Repeat tensor along each dimension.\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a34fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_tensor(x: torch.Tensor, *sizes: int) -> torch.Tensor:\n",
    "    \"\"\"Expand tensor to a larger size without copying data.(Sizes can be -1 to keep original dimension.)\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5120093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum_over_dim(x: torch.Tensor, dim: int = 0) -> torch.Tensor:\n",
    "    \"\"\"Cumulative sum along dim.\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa8c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_select(mask: torch.Tensor, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Elementwise select: return a where mask is True else b. mask must be broadcastable to a and b.\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(indices: torch.Tensor, num_classes: int, dtype: torch.dtype | None = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create one-hot encodings.\n",
    "    Output is a tensor of the same shape as indices with an added dimension of size num_classes at the end, \n",
    "    where the value along that dimension is 1 if it matches the index and 0 otherwise.\n",
    "\n",
    "    Shapes:\n",
    "    - indices: (...,) integer tensor\n",
    "    Return:\n",
    "    - out: (..., num_classes)\n",
    "\n",
    "    Requirements:\n",
    "    - Must work for arbitrary leading shape.\n",
    "    - No Python loops.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_add_1d(\n",
    "    values: torch.Tensor, indices: torch.Tensor, size: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sum `values` into an output vector at positions `indices`.\n",
    "\n",
    "    Shapes:\n",
    "    - values: (N,)\n",
    "    - indices: (N,) integer indices in [0, size)\n",
    "    Return:\n",
    "    - out: (size,) with same dtype and device as values\n",
    "\n",
    "    Requirement:\n",
    "    - no Python loops\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_token_histogram(tokens: torch.Tensor, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Count token occurrences per batch item.\n",
    "\n",
    "    Shapes:\n",
    "    - tokens: (B, T) int64\n",
    "    Return:\n",
    "    - counts: (B, vocab_size) where counts[b, v] = number of times token v appears in tokens[b] \n",
    "\n",
    "    Requirements:\n",
    "    - No Python loops over B or T.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c263a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Mean over `dim` considering only mask==True entries.\n",
    "\n",
    "    Convention:\n",
    "    - mask: bool tensor broadcastable to x\n",
    "    - mask==True means \"keep this entry\"\n",
    "\n",
    "    Return: same shape as x.mean(dim=dim)\n",
    "\n",
    "    Requirements:\n",
    "    - Avoid division by zero: if all mask are False along `dim`, define mean as 0.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63636e30",
   "metadata": {},
   "source": [
    "## Einsum warmup\n",
    "Now that you’re comfortable with shapes and broadcasting, we’ll introduce `torch.einsum`, a concise way to express tensor operations by explicitly naming axes and summing over repeated indices.\n",
    "\n",
    "### The idea\n",
    "You describe each input tensor by labeling its dimensions with letters, e.g.\n",
    "- `x: (B, T, D)` → `\"btd\"`\n",
    "- `W: (D, H)`    → `\"dh\"`\n",
    "\n",
    "Then you tell einsum what output labels you want:\n",
    "- `\"btd,dh->bth\"`\n",
    "\n",
    "### Rules of einsum\n",
    "1) **Same letter = same axis** (must match in size, except broadcastable size-1).\n",
    "2) **Repeated letters are summed over** (a “contraction”).\n",
    "3) **Letters that appear in the output are kept** (in that order).\n",
    "4) You can **reorder axes** just by changing the output label order.\n",
    "\n",
    "### Tiny cheat sheet\n",
    "- Sum over an axis: `\"btd->bt\"` (sums over `d`)\n",
    "- Transpose: `\"ij->ji\"`\n",
    "- Dot product: `\"d,d->\"` or batched `\"btd,btd->bt\"`\n",
    "- Matrix multiply: `\"ik,kj->ij\"`\n",
    "- Batched matmul: `\"bij,bjk->bik\"`\n",
    "- Outer product: `\"i,j->ij\"`\n",
    "\n",
    "### How to derive an einsum (recommended workflow)\n",
    "1) Write down shapes with named axes (e.g. `q: b h t d`, `k: b h s d`).\n",
    "2) Decide which axes you want to **sum over** (give them the same letter in both inputs).\n",
    "3) Decide which axes you want to **keep** in the output (write them after `->`).\n",
    "\n",
    "In this section, you’ll use einsum to implement building blocks that show up in attention:\n",
    "- linear projections (`x @ W`)\n",
    "- dot products\n",
    "- attention score matrices (`QKᵀ`)\n",
    "- applying attention weights (`softmax(scores) @ V`)\n",
    "\n",
    "NOTE: For these exercises you are required to use `torch.einsum` not `matmul` (we check). You are also not required to understand the attention mechanism at this point and the exercises are sovable without. It is good however, to remember the implementations in this exercise for future implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def einsum_linear_btd_dh_to_bth(x: torch.Tensor, W: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Linear projection using einsum.\n",
    "\n",
    "    Shapes:\n",
    "    - x: (B, T, D)\n",
    "    - W: (D, H)\n",
    "    Return:\n",
    "    - y: (B, T, H)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62873ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def einsum_pairwise_dot(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pairwise dot product between x and y.\n",
    "\n",
    "    Shapes:\n",
    "    - x: (B, T, D)\n",
    "    - y: (B, T, D)\n",
    "    Return:\n",
    "    - dots: (B, T) where dots[b,t] = dot(x[b,t], y[b,t])\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def einsum_qk_scores(q: torch.Tensor, k: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute attention scores QK^T using einsum.\n",
    "\n",
    "    Shapes:\n",
    "    - q: (B, H, T, Dh)\n",
    "    - k: (B, H, T, Dh)\n",
    "    Return:\n",
    "    - scores: (B, H, T, T) where scores[b,h,i,j] = dot(q[b,h,i], k[b,h,j])\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def einsum_apply_attention(weights: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply attention weights to values using einsum.\n",
    "\n",
    "    Shapes:\n",
    "    - weights: (B, H, T, T)\n",
    "    - v:       (B, H, T, Dh)\n",
    "    Return:\n",
    "    - out:     (B, H, T, Dh) where out[b,h,i] = sum_j weights[b,h,i,j] * v[b,h,j]\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3519de",
   "metadata": {},
   "source": [
    "## Attention Fundamentals\n",
    "This exercise introduces some building blocks of the attention mechanism which we will encounter extensively throughout the course. It's not yet required for you to fully understand the mechanism to implement the exercises. However, it's good to remember these building blocks for the future. \n",
    "\n",
    "To complete the exercises you should familiarize yourself with these topics:\n",
    "- Stable softmax read: https://jaykmody.com/blog/stable-softmax/\n",
    "- Masking: typically this means setting masked logits to -inf *before* softmax.\n",
    "- For attention: causal masks are upper-triangular (no attending to the future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically stable softmax along `dim`.\n",
    "\n",
    "    Requirements:\n",
    "    - Must not overflow for large values in x.\n",
    "    - Output sums to 1 along `dim`.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38475c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_fill_tensor(x: torch.Tensor, mask: torch.Tensor, value: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return a copy of x where positions with mask == True are replaced by `value`.\n",
    "    \n",
    "    Requirements:\n",
    "    - mask must be broadcastable to x.\n",
    "    - do NOT modify x in-place.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70896023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(x: torch.Tensor, mask: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Softmax over x with a boolean mask.\n",
    "\n",
    "    Convention:\n",
    "    - mask == True means \"invalid and must receive probability 0\".\n",
    "    - Do masking before softmax (i.e., set invalid logits to a large negative).”\n",
    "\n",
    "    Requirements:\n",
    "    - Must be numerically stable.\n",
    "    - Output must be exactly 0 where mask==True.\n",
    "    - If all entries are masked along `dim`, return all zeros along `dim`.\n",
    "    - You may reuse functions you implemented above.\n",
    "    \"\"\"\n",
    "    # TODO: implement using masked_fill_tensor + stable_softmax\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_causal_mask(T: int, device: torch.device | str | None = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (future-masking) boolean mask of shape (T, T).\n",
    "\n",
    "    Convention:\n",
    "    - mask[i, j] == True  => position (i attends to j) is NOT allowed (j is in the future)\n",
    "    - mask[i, j] == False => allowed\n",
    "\n",
    "    So this is an upper-triangular mask above the diagonal.\n",
    "\n",
    "    Return:\n",
    "    - mask: boolean tensor on the specified device\n",
    "\n",
    "    Example (T=4):\n",
    "        [[F, T, T, T],\n",
    "         [F, F, T, T],\n",
    "         [F, F, F, T],\n",
    "         [F, F, F, F]]\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_causal_mask(attn_logits: torch.Tensor, value: float = -1e9) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply a causal mask to attention logits.\n",
    "\n",
    "    Expected shapes:\n",
    "    - attn_logits: (..., T, T)\n",
    "\n",
    "    Returns:\n",
    "    - masked logits (same shape) where masked positions have been set to `value`.\n",
    "\n",
    "    Notes:\n",
    "    - Create a causal mask for the final two dims.\n",
    "    - Broadcast it across leading dims.\n",
    "    - You may reuse functions declared above.\n",
    "    \"\"\"\n",
    "    # TODO: implement using make_causal_mask + masked_fill_tensor\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb39ba4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotl kernel",
   "language": "python",
   "name": "robotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
